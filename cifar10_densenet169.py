# -*- coding: utf-8 -*-
"""CIFAR10_densenet169.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bzqYCWWlgfr1St8I6i3zeVqPpO4o7u6R

```
# Dataset Used - Cifar10

```
```
# Pretrained Model Used- Densenet169
```
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
print('GPU located at: {}'.format(device_name))
print('Tensorflow Version: {}'.format(tf.__version__))
print("Number of GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print(tf.config.list_physical_devices('GPU'))

# Commented out IPython magic to ensure Python compatibility.
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.utils import np_utils
from tensorflow.keras.datasets import cifar10
from tensorflow.keras import datasets, layers, models
# %matplotlib inline

def preprocess_data(X, Y):
    X = tf.keras.applications.densenet.preprocess_input(X)
    Y = tf.keras.utils.to_categorical(Y)
    return X, Y

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train, y_train = preprocess_data(X_train, y_train)
x_test, y_test = preprocess_data(X_test, y_test)
X_train.shape

print('Training images: {}'.format(X_train.shape))
print('Testing images: {}'.format(X_test.shape))

#Normalization
X_train = X_train / 255.0
X_test = X_test / 255.0

initializer = tf.initializers.he_normal()
input = tf.keras.Input(shape=(32, 32, 3))
resized_images = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224)))(input)
model = tf.keras.applications.DenseNet169(include_top=False,
                                   weights='imagenet',
                                   input_tensor=resized_images,
                                   input_shape=(224, 224, 3),
                                   classes=1000)
# Freezing each layer of the densenet169
for i in model.layers:
    i.trainable = False
output = model.layers[-1].output
flatten = tf.keras.layers.Flatten()
output = flatten(output)
layer_256 = tf.keras.layers.Dense(units=256,
                           activation='relu',
                           kernel_initializer=initializer,
                           kernel_regularizer=tf.keras.regularizers.l2())
output = layer_256(output)
dropout = tf.keras.layers.Dropout(0.5)
output = dropout(output)
softmax = tf.keras.layers.Dense(units=10,
                         activation='softmax',
                         kernel_initializer=initializer,
                         kernel_regularizer=tf.keras.regularizers.l2())
output = softmax(output)
model = tf.keras.models.Model(inputs=input, outputs=output)

model.summary()

model.compile(
         optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
         loss='categorical_crossentropy',
         metrics=['accuracy'])

lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',
                                          factor=0.6,
                                          patience=2,
                                          verbose=1,
                                          mode='max',
                                          min_lr=1e-7)
checkpoint = tf.keras.callbacks.ModelCheckpoint('cifar10_densenet169.h5',
                                         monitor='val_accuracy',
                                         verbose=1,
                                         save_weights_only=False,
                                         save_best_only=True,
                                         mode='max',
                                         save_freq='epoch')

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(                                          
                                          horizontal_flip=True,                                        
                                          )
train_generator = train_datagen.flow(x_train,
                                     y_train,
                                     batch_size=32)
val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(                                  
                                        horizontal_flip=True,                                        
                                        )
val_generator = val_datagen.flow(x_test, y_test, batch_size=32)
train_steps_per_epoch = x_train.shape[0] // 32
val_steps_per_epoch = x_test.shape[0] // 32
history = model.fit(train_generator, steps_per_epoch=train_steps_per_epoch,
                    validation_data=val_generator,
                    validation_steps=val_steps_per_epoch,
                    epochs=20,
                    shuffle=True,
                    callbacks=[lr_reduce, checkpoint],
                    verbose=1)

#train set accuracy
model.evaluate(X_train,y_train)

#test set accuracy
model.evaluate(X_test,y_test)

from google.colab import drive
drive.mount('/content/drive')

model.save("/content/drive/MyDrive/cifar10__densenet169.h5")

plt.figure(figsize=(10,6))
plt.plot(history.history['accuracy'], 'b')
plt.plot(history.history['val_accuracy'],'g')
plt.title('Accuracy Graph')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
print("\n")

plt.figure(figsize=(10,6))
plt.plot(history.history['loss'], 'b')
plt.plot(history.history['val_loss'], 'g')
plt.title('Loss Graph')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

